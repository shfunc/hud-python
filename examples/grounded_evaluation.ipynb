{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation with Grounded Agent\n",
    "\n",
    "This notebook shows how to run evaluations using the GroundedOpenAIChatAgent on OSWorld.\n",
    "\n",
    "The grounded agent separates visual grounding from reasoning:\n",
    "- **Planning model** (GPT-4o-mini): High-level reasoning and task planning\n",
    "- **Grounding model** (Qwen2.5-VL): Visual element detection and coordinate resolution\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Set `HUD_API_KEY` in your environment\n",
    "- Set `OPENAI_API_KEY` for the planning model\n",
    "- Set `OPENROUTER_API_KEY` for the grounding model (or use local grounding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install hud-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "import hud\n",
    "from datasets import load_dataset\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "from hud.agents.grounded_openai import GroundedOpenAIChatAgent\n",
    "from hud.tools.grounding.config import GrounderConfig\n",
    "from hud.settings import settings\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(name)s - %(message)s\", datefmt=\"%H:%M:%S\"\n",
    ")\n",
    "logging.getLogger(\"hud.agents\").setLevel(logging.INFO)\n",
    "\n",
    "# Disable httpx logging to reduce noise\n",
    "logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"httpcore\").setLevel(logging.WARNING)\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set up API keys and model configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Keys - make sure these are set in your environment\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\") or settings.openai_api_key\n",
    "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\") or settings.openrouter_api_key\n",
    "HUD_API_KEY = os.getenv(\"HUD_API_KEY\")\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found. Please set it in your environment.\")\n",
    "if not OPENROUTER_API_KEY:\n",
    "    raise ValueError(\"OPENROUTER_API_KEY not found. Please set it in your environment.\")\n",
    "if not HUD_API_KEY:\n",
    "    raise ValueError(\"HUD_API_KEY not found. Please set it in your environment.\")\n",
    "\n",
    "print(\"âœ… API keys configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Grounded Agent Configuration\n",
    "\n",
    "You can configure the grounded agent to work with `run_dataset` for full evaluations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grounding model configuration\n",
    "grounder_config = GrounderConfig(\n",
    "    api_key=OPENROUTER_API_KEY,\n",
    "    api_base=\"https://openrouter.ai/api/v1\",\n",
    "    model=\"qwen/qwen-2.5-vl-7b-instruct\",\n",
    ")\n",
    "\n",
    "# OpenAI client for planning model\n",
    "openai_client = AsyncOpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Agent configuration for dataset runner\n",
    "agent_class = GroundedOpenAIChatAgent\n",
    "agent_config = {\n",
    "    \"grounder_config\": grounder_config,\n",
    "    \"openai_client\": openai_client,\n",
    "}\n",
    "\n",
    "print(\"âœ… Agent configuration ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Task Test\n",
    "\n",
    "First, let's test the grounded agent on a single OSWorld task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_single_task(\n",
    "    dataset_name: str,\n",
    "    task_index: int = 1,\n",
    "    max_steps: int = 10,\n",
    ") -> None:\n",
    "    \"\"\"Load one task from dataset_name and execute it.\"\"\"\n",
    "\n",
    "    print(\"ðŸ“Š Loading datasetâ€¦\")\n",
    "    dataset = load_dataset(dataset_name, split=\"train\")\n",
    "\n",
    "    # Get a task from dataset\n",
    "    sample_task = dataset[task_index]\n",
    "    task_prompt = sample_task.get(\"prompt\", f\"Task {sample_task.get('id', 0)}\")\n",
    "\n",
    "    with hud.trace(name=task_prompt):\n",
    "        task = Task(**sample_task)\n",
    "\n",
    "        # Create agent with configuration\n",
    "        agent = agent_class(**agent_config)\n",
    "        agent.metadata = {}\n",
    "\n",
    "        print(f\"\\nðŸŽ¯ Task: {task.prompt}\")\n",
    "        result = await agent.run(task, max_steps=max_steps)\n",
    "        print(\"âœ… Reward:\", result.reward)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test single task\n",
    "result = await run_single_task(\"hud-evals/OSWorld-Verified\", task_index=1, max_steps=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Dataset Evaluation\n",
    "\n",
    "Now let's run evaluations on complete datasets using the factory functions from `hud.utils.agent_factories`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hud.datasets import run_dataset\n",
    "from hud.utils.agent_factories import create_grounded_agent\n",
    "\n",
    "# Configuration for the factory functions\n",
    "grounded_agent_config = {\n",
    "    \"api_key\": OPENAI_API_KEY,\n",
    "    \"grounder_api_key\": OPENROUTER_API_KEY,\n",
    "    \"grounder_api_base\": \"https://openrouter.ai/api/v1\",\n",
    "    \"grounder_model\": \"qwen/qwen-2.5-vl-7b-instruct\",\n",
    "    \"model_name\": \"gpt-4o-mini\",\n",
    "}\n",
    "\n",
    "print(\"âœ… Factory configurations ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Small Dataset Evaluation\n",
    "\n",
    "For smaller datasets (< 100 tasks), use the standard `run_dataset` with async concurrency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load OSWorld dataset and take a subset for evaluation\n",
    "dataset = load_dataset(\"hud-evals/OSWorld-Verified\", split=\"train\")\n",
    "\n",
    "# Take first 30 tasks for evaluation\n",
    "subset_size = 30\n",
    "task_subset = dataset[:subset_size]\n",
    "\n",
    "\n",
    "task_list = []\n",
    "for i in range(len(task_subset[\"prompt\"])):\n",
    "    task_dict = {key: task_subset[key][i] for key in task_subset.keys()}\n",
    "    task_list.append(task_dict)\n",
    "\n",
    "print(f\"ðŸ“Š Loaded {len(task_list)} tasks from OSWorld-Verified\")\n",
    "print(f\"First task: {task_list[0].get('prompt', 'No prompt')[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation on the OSWorld subset with grounded agent\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "results = await run_dataset(\n",
    "    name=\"OSWorld-30 Grounded Eval\",\n",
    "    dataset=task_list,  # Pass the list of task dicts\n",
    "    agent_class=create_grounded_agent,  # Use factory function\n",
    "    agent_config=grounded_agent_config,\n",
    "    max_concurrent=10,  # Moderate concurrency for 30 tasks\n",
    "    max_steps=15,\n",
    "    auto_respond=True,  # Auto-continue agent\n",
    "    metadata={\"model\": \"gpt-4o-mini\", \"grounding\": \"qwen-2.5-vl\", \"dataset\": \"OSWorld-30\"},\n",
    ")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "# Calculate statistics\n",
    "successful = sum(1 for r in results if getattr(r, \"reward\", 0) > 0)\n",
    "failed = sum(1 for r in results if getattr(r, \"isError\", False))\n",
    "total = len(results)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ðŸ“Š OSWorld-30 Evaluation Complete!\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total tasks: {total}\")\n",
    "print(f\"âœ… Successful: {successful} ({100 * successful / total:.1f}%)\")\n",
    "print(f\"âŒ Failed: {failed} ({100 * failed / total:.1f}%)\")\n",
    "print(f\"â±ï¸ Time elapsed: {elapsed:.2f} seconds\")\n",
    "print(f\"ðŸ“ˆ Throughput: {total / elapsed:.2f} tasks/second\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Large Dataset Evaluation (Parallel)\n",
    "\n",
    "For larger datasets (100+ tasks), use `run_dataset_parallel` for process-based parallelization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run full OSWorld evaluation with parallel execution and configured workers\n",
    "from hud.datasets import run_dataset_parallel_manual\n",
    "\n",
    "# Uncomment to run with parallel execution\n",
    "\"\"\"\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "results = await run_dataset_parallel_manual(\n",
    "    name=\"OSWorld Parallel Eval\",\n",
    "    dataset=\"hud-evals/OSWorld-Verified\",  # 300+ tasks\n",
    "    agent_class=create_grounded_agent,\n",
    "    agent_config=grounded_agent_config,\n",
    "    max_workers=8,                      # Number of worker processes\n",
    "    max_concurrent_per_worker=10,       # Concurrent tasks per worker (8*10 = 80 total)\n",
    "    max_steps=15,\n",
    "    auto_respond=True,\n",
    "    metadata={\"model\": \"gpt-4o-mini\", \"grounding\": \"qwen-2.5-vl\", \"parallel\": True}\n",
    ")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "# Print statistics\n",
    "print(\"\\\\n\" + \"=\" * 50)\n",
    "print(\"ðŸ“Š Evaluation Complete!\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total tasks: {len(results)}\")\n",
    "print(f\"Time elapsed: {elapsed:.2f} seconds\")\n",
    "print(f\"Throughput: {len(results) / elapsed:.2f} tasks/second\")\n",
    "print(f\"Execution mode: PARALLEL (workers: 8, concurrent per worker: 10)\")\n",
    "\n",
    "successful = sum(1 for r in results if getattr(r, \"reward\", 0) > 0)\n",
    "print(f\"Successful tasks: {successful}/{len(results)} ({100 * successful / len(results):.1f}%)\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"Ready to run large parallel evaluation with configured workers (uncomment code above)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hud-python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
