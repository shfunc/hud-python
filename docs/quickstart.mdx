---
title: "Quickstart"
description: "Run your first agent evaluation in 3 minutes"
icon: "bolt"
---

Get up and running with HUD in minutes. This guide walks you through installation, basic setup, and running your first agent evaluation.

## Quick Clone

The fastest way to get started:

```bash
# Clone a complete example project with uv
uvx hud-python quickstart
```

This sets you up with a working agent evaluation example you can run immediately.

## Installation

<Tabs>
<Tab title="Basic Install">
```bash
pip install hud-python
```
</Tab>

<Tab title="Agent Install">
```bash
# Includes AI providers and telemetry
pip install "hud-python[agent]"
```
</Tab>

<Tab title="CLI Tools">
```bash
# Install CLI in isolated environment
uv tool install hud-python
```
</Tab>
</Tabs>

## API Keys

Set your API keys as environment variables:

```bash
export HUD_API_KEY="sk-hud-..."      # Get from app.hud.so
export ANTHROPIC_API_KEY="sk-ant-..." # For Claude agents
export OPENAI_API_KEY="sk-..."        # For OpenAI agents
```

<Tip>
Create a `.env` file in your project root to manage API keys locally
</Tip>

## Your First Agent

Run an agent on the 2048 game environment:

```python
import asyncio, os
import hud
from hud.datasets import Task
from hud.agents import ClaudeAgent

async def main():
    # The trace context captures ALL agent interactions within a "task run"
    # Everything inside this context shows up as one trace on app.hud.so
    with hud.trace("quickstart-2048"):
        # Define task with remote MCP environment
        task = Task(
            prompt="Win a game of 2048 by reaching the 128 tile",
            mcp_config={
                "hud": {
                    "url": "https://mcp.hud.so/v3/mcp",
                    "headers": {
                        "Authorization": f"Bearer {os.getenv('HUD_API_KEY')}",
                        "Mcp-Image": "hudevals/hud-text-2048:0.1.3"
                    }
                }
            },
        setup_tool={"name": "setup", "arguments": {"name": "board", "arguments": { "board_size": 4}}},
        evaluate_tool={"name": "evaluate", "arguments": {"name": "max_number", "arguments": {"target": 64}}}
        )
        
        # Run agent (auto-creates MCP client from task.mcp_config)
        agent = ClaudeAgent()
        result = await agent.run(task)
        print(f"Max tile reached: {result.reward}")

asyncio.run(main())
```

<Check>
The trace context ensures that the entire task run - from setup through evaluation - appears as one coherent trace on the platform
</Check>

## What just happened?

1. **Task Definition**: We created a `Task` with:
   - A prompt telling the agent what to do
   - An `mcp_config` pointing to a remote MCP environment
   - Setup and evaluation tools to initialize and score the game

2. **Auto Client**: The agent automatically created an MCP client from `task.mcp_config`

3. **Telemetry**: The `trace` context captured all interactions for debugging

4. **Evaluation**: The `evaluate_max_tile` tool returned the highest tile as reward

## Next Steps

<CardGroup cols={3}>
<Card title="Understand MCP" icon="book" href="/core-concepts/mcp-protocol">
  Learn how agents connect to environments
</Card>

<Card title="Run Benchmarks" icon="chart-line" href="/evaluate-agents/run-benchmarks">
  Test on SheetBench and OSWorld
</Card>

<Card title="Build Environments" icon="cube" href="/build-environments/quickstart-todo">
  Create your own MCP environment
</Card>
</CardGroup>

## CLI Quick Reference

```bash
# Analyze any MCP environment
hud analyze hudevals/hud-text-2048:0.1.3

# Debug environment initialization
hud debug hudevals/hud-text-2048:0.1.3

# Start hot-reload development server
hud dev . --build
```

<Note>
See the [CLI Reference](/reference/cli/overview) for detailed command documentation
</Note>

