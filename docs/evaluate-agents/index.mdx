---
title: "Evaluate Agents"
description: "Test and benchmark your agents on standardized tasks"
icon: "flask-vial"
---

HUD makes it easy to evaluate any MCP-compatible agent on a variety of tasks and benchmarks. Whether you're testing Claude, Operator, or your custom agent, the evaluation flow is consistent.

## Quick Start

Evaluate an agent on a single task:

```python
import asyncio
import hud
import os
from hud.datasets import Task
from hud.agents import ClaudeAgent

async def main():
    with hud.trace("eval-demo"):
        task = Task(
            prompt="Start at the Wikipedia page for 'Python programming language' and navigate to the page for 'Mount Everest' only through page links.",
            mcp_config={
                "hud": {
                    "url": "https://mcp.hud.so/v3/mcp",
                    "headers": {
                        "Authorization": f"Bearer {os.getenv('HUD_API_KEY')}",
                        "Mcp-Image": "hudevals/hud-remote-browser:0.1.1"
                    }
                }
            },
            setup_tool={
                "name": "setup",
                "arguments": {
                    "name": "navigate_to_url",
                    "arguments": {"url": "https://en.wikipedia.org/wiki/Python_(programming_language)"}
                }
            },
            evaluate_tool={
                "name": "evaluate",
                "arguments": {
                    "name": "page_contains",
                    "arguments": {
                        "search_terms": [
                            "Mount Everest",
                            "8,848",
                            "Himalayas",
                            "Nepal",
                            "China"
                        ],
                        "partial_rewarding": True
                    }
                }
            }
        )
        
        agent = ClaudeAgent(
            allowed_tools=["anthropic_computer"]
        )
        result = await agent.run(task)
        print(f"Success: {result.reward > 0.5}")

if __name__ == "__main__":
    asyncio.run(main())

```

## What You Can Do

### 1. Create Tasks for Browser Automation

```python
import os

task = Task(
    prompt="Navigate to GitHub and star the HUD repository",
    mcp_config={
        "hud": {
            "url": "https://mcp.hud.so/v3/mcp",
            "headers": {
                "Authorization": f"Bearer {os.getenv('HUD_API_KEY')}",
                "Mcp-Image": "hudevals/hud-remote-browser:0.1.1"
            }
        }
    },
    setup_tool={
        "name": "setup",
        "arguments": {
            "name": "navigate",
            "arguments": {"url": "https://github.com"}
        }
    },
    evaluate_tool={
        "name": "evaluate", 
        "arguments": {
            "name": "url_match",
            "arguments": {
                "target_url": "hud-python"
            }
        }
    }
)
```

### 2. Run Existing Benchmarks

```python
from datasets import load_dataset
from hud.datasets import run_dataset_parallel

# Load and run SheetBench-50 (parallel execution)
dataset = load_dataset("hud-evals/sheetbench-50", split="train")
results = await run_dataset_parallel(
    "My SheetBench Run",
    dataset,
    agent_class=ClaudeAgent
)
```

### 3. Make Your Agent Work with HUD

To create a custom agent, inherit from `MCPAgent` and implement the required methods:

```python
from hud.agents import MCPAgent
from hud.types import AgentResponse, MCPToolCall

class MyCustomAgent(MCPAgent):
    async def get_response(self, messages: list[Any]) -> AgentResponse:
        # Call your LLM and return tool calls
        ...
    
    async def format_blocks(self, blocks: list[Any]) -> list[Any]:
        # Format content blocks into messages for your LLM
        ...
    
    async def format_tool_results(
        self, tool_calls: list[MCPToolCall], 
        tool_results: list[Any]
    ) -> list[Any]:
        # Format tool results back into messages
        ...

# Now it works with any HUD dataset!
```

<Card title="Full Implementation Guide" icon="code" href="/evaluate-agents/create-agents">
  See complete examples and implementation details
</Card>

## Available Benchmarks

<CardGroup cols={2}>
<Card title="SheetBench-50" icon="table">
  50 real-world spreadsheet tasks testing data manipulation, formulas, and analysis
</Card>

<Card title="OSWorld-Verified" icon="desktop">
  Desktop automation tasks across Ubuntu applications
</Card>

<Card title="OnlineMind2Web" icon="globe">
  Web navigation and interaction challenges (Coming Soon)
</Card>

<Card title="2048 Puzzles" icon="gamepad">
  Strategic planning in the 2048 game environment
</Card>
</CardGroup>

## Exploring Environments

Use `hud analyze` to discover available tools and evaluators:

```bash
$ hud analyze hudevals/hud-remote-browser:0.1.1

ğŸ” Analyzing hudevals/hud-remote-browser:0.1.1...

ğŸ“Š Environment Summary:
â”œâ”€â”€ Tools: 15 available
â”œâ”€â”€ Setup Functions: 8 available  
â”œâ”€â”€ Evaluators: 12 available
â””â”€â”€ Resources: 3 available

ğŸ› ï¸ Tools:
â”œâ”€â”€ playwright(action: str, **kwargs) - Browser automation actions
â”œâ”€â”€ click(selector: str) - Click element
â”œâ”€â”€ type(selector: str, text: str) - Type text
â””â”€â”€ ... 12 more tools

ğŸ“‹ Evaluators:
â”œâ”€â”€ url_contains(substring: str) - Check if URL contains text
â”œâ”€â”€ page_contains(text: str, regex: bool = False) - Check page content
â”œâ”€â”€ element_exists(selector: str) - Check if CSS selector exists
â”œâ”€â”€ todo_completed(expected_count: int) - Verify TODO completion
â””â”€â”€ ... 8 more evaluators

Run with --json for full details or pipe to grep for filtering.
```

## Publishing to Leaderboards

After running evaluations, view results on the leaderboard:

```python
# Run evaluation (parallel)
results = await run_dataset_parallel(
    "Claude-4 Sonnet SheetBench",
    dataset="hud-evals/sheetbench-50",
    agent_class=ClaudeAgent
)

# Then visit: app.hud.so/leaderboards/hud-evals/sheetbench-50
# Click "My Jobs" to see your runs and create scorecards
```

## Key Features

- **Reproducible**: Docker environments ensure consistency
- **Parallel**: Run multiple evaluations concurrently
- **Observable**: Every tool call tracked with telemetry
- **Extensible**: Easy to add new tasks or benchmarks

## Next Steps

<CardGroup cols={3}>
<Card title="Create Agents" icon="robot" href="/evaluate-agents/create-agents">
  Build your own MCP-compatible agent
</Card>

<Card title="Leaderboards" icon="trophy" href="/evaluate-agents/leaderboards">
  Track and compare agent performance
</Card>

<Card title="Create Benchmarks" icon="flask" href="/evaluate-agents/create-benchmarks">
  Build custom evaluation datasets
</Card>
</CardGroup>

